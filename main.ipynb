{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Florida free-speech project\n",
    "\n",
    "The goal of the project is to get transcripts from Town Hall meetings in Florida cities and towns for research purposes. \n",
    "\n",
    "In practice, from a given list of towns in Florida, I used YouTube API to first search for official town/city channel. The I asked ChatGPT to evaluate if the channel seems official based on a channel title and description. Lastly, I called YouTube API to get all videos from the channel and get transcripts for each video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The repo is called [transcripts](https://github.com/nesaboz/transcripts):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_github_code():\n",
    "    # First get GitHub code:\n",
    "    !wget https://github.com/nesaboz/transcripts/archive/refs/heads/main.zip\n",
    "    # unzip it\n",
    "    !unzip main.zip\n",
    "    # copy all the files to root\n",
    "    !mv ./transcripts-main/* .\n",
    "    # delete the empty folder\n",
    "    !rm -r transcripts-main\n",
    "    # delete zip file\n",
    "    !rm main.zip\n",
    "    # delete main.ipynb since it's confusing to have it Colab:\n",
    "    !rm main.ipynb\n",
    "\n",
    "def install_packages():\n",
    "    !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    IS_COLAB = True\n",
    "except ModuleNotFoundError:\n",
    "    IS_COLAB = False\n",
    "\n",
    "\n",
    "if IS_COLAB: \n",
    "    response = input(\"Do you want to setup everything? ([yes]/no): \").lower().strip()\n",
    "    if response != \"no\":\n",
    "        !rm -r sample_data  # delete sample_data for beauty\n",
    "        get_github_code()\n",
    "        drive.mount('/content/drive')\n",
    "        install_packages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from utils import ChannelCrawler, ChannelAnalyzer, aggregate_analysis_files, Channel, VideoInfo\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for YT channels\n",
    "\n",
    "So we go over the list of all cities in Florida and search YouTube for \"city of XYZ Florida\" and \"town of XYZ Florida\". This is what `Crawler` class does. See docstring in `Crawler` for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File status.csv not found. Creating a new one.\n"
     ]
    }
   ],
   "source": [
    "crawler = ChannelCrawler(search_query_fns=[lambda x: f\"town of {x}, Florida\", lambda x: f\"city of {x}, Florida\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now start crawling, limit is infinite by default though you will of course hit into YouTube API quota limit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler.start(limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all goes well one should have folder called `responses` in the root."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each json response in `responses` we will now ask ChatGPT to determine whether the channel is official or not. This will a new folder `analysis` with csv files having yes/no answers, and updates in `status.csv`. We first create analyzer and then run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = ChannelAnalyzer(\n",
    "    model_name=\"gpt-4\",  # \"gpt-3.5-turbo\"\n",
    "    prompt_fn= lambda x: f\"Your job will be to analyze a short text, \\\n",
    "comprised of a title and a description of a YouTube channel, to asses whether this \\\n",
    "text corresponds to an official YouTube channel of a city {x}, in Florida. Your answer should be 'Yes' or 'No' only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now aggregate the results in an excel file, very similar to the `assets/cities_to_collect.xlsx`, storing only positive results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_analysis_files(crawler, 'aggregated_analysis.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all live videos from one channel\n",
    "\n",
    "We now feed in channel id and get all the live videos from that channel. Let's take an example of city of Belleair Beach (this city is NOT in the list of cities provided):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel = Channel('UCBTiCuq7bdOfOjqAnHY0zbA')\n",
    "channel = Channel('UCm9YZSpPqHckVrtDdrL3isw')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel.get_videos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract info from one video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = VideoInfo(\"thGB9IILDOw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video.get_all_video_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract all transcripts from a channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel.extract_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = VideoInfo('3eHSnYwnX4g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video.get_only_transcript()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
