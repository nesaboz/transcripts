{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Florida free-speech project\n",
    "\n",
    "The goal of the project is to get transcripts from Town Hall meetings in Florida cities and towns for research purposes. \n",
    "\n",
    "In practice, from a given list of towns in Florida, I used YouTube API to first search for official town/city channel. The I asked ChatGPT to evaluate if the channel seems official based on a channel title and description. Lastly, I called YouTube API to get all videos from the channel and get transcripts for each video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The repo is called [transcripts](https://github.com/nesaboz/transcripts):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    IS_COLAB = True\n",
    "except ModuleNotFoundError:\n",
    "    IS_COLAB = False\n",
    "\n",
    "\n",
    "if IS_COLAB: \n",
    "    response = input(\"Do you want to setup everything? ([yes]/no): \").lower().strip()\n",
    "    if response != \"no\":\n",
    "        # delete sample_data folder for beauty\n",
    "        !rm -rf /content/* /content/.[!.]* /content/..?*\n",
    "\n",
    "        !git clone https://github.com/nesaboz/transcripts.git /content\n",
    "\n",
    "        # mount google drive\n",
    "        drive.mount('/content/drive')\n",
    "        \n",
    "        # install_packages\n",
    "        !pip install -r requirements.txt\n",
    "\n",
    "        !cp \"drive/MyDrive/.env\" .\n",
    "\n",
    "        DATA_FOLDER = \"drive/MyDrive/PN\"\n",
    "else:\n",
    "    !pip install -r requirements.txt\n",
    "    \n",
    "    DATA_FOLDER = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up data folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ChannelCrawler, ChannelAnalyzer, aggregate_analysis_files, Channel, VideoInfo\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for YT channels\n",
    "\n",
    "So we go over the list of all cities in Florida and search YouTube for \"city of XYZ Florida\" and \"town of XYZ Florida\". This is what `Crawler` class does. See docstring in `Crawler` for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler = ChannelCrawler(search_query_fns=[lambda x: f\"town of {x}, Florida\", lambda x: f\"city of {x}, Florida\"], data_folder=DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now start crawling, limit is infinite by default though you will of course hit into YouTube API quota limit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler.start(limit=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all goes well one should have folder called `responses` in the data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each json response in `responses` we will now ask ChatGPT to determine whether the channel is official or not. This will a new folder `analysis` with csv files having yes/no answers, and updates in `status.csv`. We first create analyzer and then run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = ChannelAnalyzer(\n",
    "    model_name=\"gpt-4\",\n",
    "    prompt_fn= lambda x: f\"Your job will be to analyze a short text, \\\n",
    "comprised of a title and a description of a YouTube channel, to asses whether this \\\n",
    "text corresponds to an official YouTube channel of a city {x}, in Florida. Your answer should be 'Yes' or 'No' only\",\n",
    "    data_folder=DATA_FOLDER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now aggregate the results in an excel file, very similar to the `assets/cities_to_collect.xlsx`, storing only positive results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_analysis_files(crawler, analyzer, DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract info from one video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we shift focus on video metadata and transcripts, each video has ID, we can simply exatract all info knowing that ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = VideoInfo(\"thGB9IILDOw\", DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video.get_all_video_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video.get_only_transcript()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract info from all videos of a channel\n",
    "\n",
    "We now know how to extract one transcript, we just need to get a list of all videos of a channel (with some id) and repeat the extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = Channel('UCm9YZSpPqHckVrtDdrL3isw', DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all videos and create a file `data/channels/<channel_id>/videos.json`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel.get_videos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all transcripts from a all channel videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel.extract_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
